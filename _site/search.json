[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Quarto Website",
    "section": "",
    "text": "Dashboard\nHello! I am a first year masters student in the Social Data Analytics and Research program.\nThis website showcases my work in Knowledge Mining Spring 2026.\nContact Information:\nPhone Number: (972) 832-1806\nWork Number: (972) 883-3743\nEmail: kmg200008@UTdallas.edu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About me!\nI am a first year masters student in the Social Data Analytics and Research program at the University of Texas at Dallas. I completed my bachelors in Psychology from the University of Texas at Arlington. After graduating, I become a lab manager for the Kennedy and Rodrigue Cognitive Neuroscience lab at UTD.\nI was interested in social data analytics because I wanted to broaden my realm of knowledge. I do not have strong coding or statistical skills, but I know that these skills would better equip me for jobs in psychology and data analysis.\nMy research interests include: The use of psychedelics on treatment resistant depression. I believe that we should expanding the number of modalities available for mental health disorders in order to help as many people as possible.\nSome fun facts about me:\n\nI am left handed\nI have one small Pug-Pomeranian dog\nI am an avid chess and Tetris player\nI enjoy archery, crocheting, and playing video games"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "assignment1",
    "section": "",
    "text": "Comparing “Statistical Modeling: The Two Cultures” and “To Explain or to Predict?”\nStatistical Modeling: The Two Cultures tackled the issue of data modeling being the primary approach in the science field, but algorithmic models are often more reliable for predictions in larger data sets and handling modern problems. For instance, Breiman discusses problems in data modeling by explaining a residual analysis where a data set is limited to two or three variables. He mentions that a goodness-of-fit test “lacks power in more than a few dimensions.” (203, Breiman) and how an acceptable plot does not mean that it is necessarily a good plot to use for data. He also explains that researchers are not usually looking for if the models they use fit the data, but if the model is new, clever, and sets them apart. Algorithmic models, although simpler, provide a higher predictive accuracy with more reliable information (214, Breiman), which is beneficial because loosely accurate results can lead to unanswered questions and questioning the research. To Explain or to Predict? discusses the differences between explanatory modeling and predictive modeling and how combining the two can cause problems because of their differences. The distinction between the two is important because researchers assume that because a model explains well, it also predicts results well but that is not always true. Shmueli explains the differences, stating, “measurable data is not an accurate representation of their underlying constructs. The operationalization of theories… creates a disparity between the ability to explain at the conceptual level and the ability to generate predictions” (293, Shmueli). Because a model can explain the data and represent the research numerically and abstractly, does not mean the conclusions drawn from the explanatory models will represent the research with the most accuracy. Similar to Breiman, good models and measurable data do not mean that they create the best predictions or the best results. Both articles challenge the idea of using complex language and models for better predictions. Breiman suggests that simpler models are more reliable and Shmueli agrees that “blending simpler models [provides] more accurate predictions” (302, Shmueli). Both articles reject the idea of over explaining concepts and using complex models for the sake of originality or sophistication. They also encourage other researchers to think more about the goals of their research and how their models and predictions fit with their research, instead of trying to have their research match the models. However, there are differences between the articles; Shmueli focuses on study design and how explanations and predictions are not related to understanding, while Breiman centers his article on the overuse of data models. Shmueli discusses why different goals matter for current and future research, whereas Breiman suggests that research practices should change and embrace algorithmic models as a reliable method of data visualization. Even though each author takes a different approach to predictions and data, they both criticize researchers who are stubborn with their approaches and who do not acknowledge future impacts of their studies.\nBreiman, Leo. 2001. “Statistical modeling: The two cultures (with comments and a rejoinder by the author)”. Statistical Science, 16(3), pp.199-231 Shmueli, Galit. 2010. “To explain or to predict?.” Statistical science 25, no. 3: 289-310"
  },
  {
    "objectID": "Breiman and Shmueli.html",
    "href": "Breiman and Shmueli.html",
    "title": "Breiman and Shmueli",
    "section": "",
    "text": "Comparing “Statistical Modeling: The Two Cultures” and “To Explain or to Predict?”\nStatistical Modeling: The Two Cultures tackled the issue of data modeling being the primary approach in the science field, but algorithmic models are often more reliable for predictions in larger data sets and handling modern problems. For instance, Breiman discusses problems in data modeling by explaining a residual analysis where a data set is limited to two or three variables. He mentions that a goodness-of-fit test “lacks power in more than a few dimensions.” (203, Breiman) and how an acceptable plot does not mean that it is necessarily a good plot to use for data. He also explains that researchers are not usually looking for if the models they use fit the data, but if the model is new, clever, and sets them apart. Algorithmic models, although simpler, provide a higher predictive accuracy with more reliable information (214, Breiman), which is beneficial because loosely accurate results can lead to unanswered questions and questioning the research.\nTo Explain or to Predict? discusses the differences between explanatory modeling and predictive modeling and how combining the two can cause problems because of their differences. The distinction between the two is important because researchers assume that because a model explains well, it also predicts results well but that is not always true. Shmueli explains the differences, stating, “measurable data is not an accurate representation of their underlying constructs. The operationalization of theories… creates a disparity between the ability to explain at the conceptual level and the ability to generate predictions” (293, Shmueli). Because a model can explain the data and represent the research numerically and abstractly, does not mean the conclusions drawn from the explanatory models will represent the research with the most accuracy.\nSimilar to Breiman, good models and measurable data do not mean that they create the best predictions or the best results. Both articles challenge the idea of using complex language and models for better predictions. Breiman suggests that simpler models are more reliable and Shmueli agrees that “blending simpler models [provides] more accurate predictions” (302, Shmueli). Both articles reject the idea of over explaining concepts and using complex models for the sake of originality or sophistication. They also encourage other researchers to think more about the goals of their research and how their models and predictions fit with their research, instead of trying to have their research match the models. However, there are differences between the articles; Shmueli focuses on study design and how explanations and predictions are not related to understanding, while Breiman centers his article on the overuse of data models. Shmueli discusses why different goals matter for current and future research, whereas Breiman suggests that research practices should change and embrace algorithmic models as a reliable method of data visualization. Even though each author takes a different approach to predictions and data, they both criticize researchers who are stubborn with their approaches and who do not acknowledge future impacts of their studies.\nBreiman, Leo. 2001. “Statistical modeling: The two cultures (with comments and a rejoinder by the author)”. Statistical Science, 16(3), pp.199-231 Shmueli, Galit. 2010. “To explain or to predict?.” Statistical science 25, no. 3: 289-310"
  },
  {
    "objectID": "AI and Originality.html",
    "href": "AI and Originality.html",
    "title": "AI and Originality",
    "section": "",
    "text": "With the emergence of AI, ChatGPT, and homework help, it has become easier to think less and have the computer work more. People are becoming more unoriginal and relying on AI and the internet to do the thinking for them. Previously when writing a research paper, a person would have to sift through articles, Google Scholar, and books to find evidence that supports their topic, but now, ChatGPT is able to populate 10 articles that directly correlate to the research in seconds. AI is decreasing our cognitive load and capabilities because it can give us precise information, ideas, answers, and work within no time. We no longer have to sit and think up research ideas; we can simply ask ChatGPT and it will have as many ideas as we want that all relate to our interests.\nOur creative endeavors have also changed with AI. A major controversy with AI is its use in art. AI creates new ideas, paintings, music compositions, videos and stories and takes away from humans doing the thinking. We don’t have to put in the mental effort to think of a new story or a new project idea because AI can generate those same ideas in a fraction of the time, saving humans time and energy. Even though those creative projects still exist and there is a huge market for it, there are also many companies and people who prefer to use AI to get the same result with half the effort.\nAi is built with an algorithm that is honestly lackluster. It is generic and the “safe option”. AI won’t give you the out-of-the-box topics or ideas you’re searching for because it is trained to appeal to the masses. And people stop searching for those far fetched ideas because AI is also training us. When we are only given the safe option or the average response, we are becoming more accustomed to accepting mediocrity and not thinking outside of what we know. Often, the “good” ideas are found through trail and error or human connection and if we are not given the opportunities to fail or connect, we won’t achieve the same results. The “safe option” can also refer to word choices. There are countless times where I have googled synonyms for “discussed” and used the first synonym to pop up instead of using my own writing style and thinking about what I would want to use. We are becoming conditioned to ignore our own thoughts and go with the “safe option” we know will be more understood.\nAGI stands for artificial general intelligence. It is an advanced form of AI that can process and learn at an above or equal level to human intelligence. Is it able to apply knowledge learned in one area to another unrelated area with ease. AGI affects scientific research because it will make it easier to connect different scientific disciplines and papers in ways that people would not normally think of. AGI will also be able to understand information, make correlations, understand causation, and design experiments to prove the theories without any effort or time. Effectively, AGI would be able to do the same thinking and designing that a human can do in no time and on a greater scale. If AGI were to be used in research, I believe it would negatively impact researchers, their jobs, and change the way research is conducted. The need for data analysts would decrease and researchers would no longer have to think up research ideas. Overall, AGI in research would drastically change the way research is conducted and change a researcher’s role to more of an overseer than a creator."
  }
]